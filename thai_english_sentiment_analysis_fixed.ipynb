{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_hhsZUL-g6i",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Thai-English Sentiment Analysis\n",
    "\n",
    "This notebook creates a sentiment analysis model that supports both Thai and English text. The model classifies text as Positive, Neutral, or Negative.\n",
    "\n",
    "Created for: ugritchaichana\n",
    "Date: 2025-04-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glMv0W9b-g6j",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cFB6ygL-g6k",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "54d9d0fb-57c8-45e8-f803-0d51ff721b81"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q pandas numpy scikit-learn pythainlp openpyxl matplotlib seaborn requests beautifulsoup4 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_BShTnr-g6k",
    "outputId": "ad5b278f-b718-4137-e86a-5a15a99637d8"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pythainlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythainlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     23\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pythainlp'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Check if we have TPU or GPU (though we'll use CPU for most operations)\n",
    "!nvidia-smi || echo \"Running on CPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU5anj7t-g6l"
   },
   "source": [
    "## 2. Load Initial Dataset (Your Current Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "xWqnghGi-g6l",
    "outputId": "d359597d-5544-41ed-9aa1-148ed6b876c3"
   },
   "outputs": [],
   "source": [
    "# Load your initial dataset\n",
    "initial_file_path = \"https://drive.google.com/uc?id=1wB6Xl44YnpJDt9YRqaIcAEw_BniL_fel\"\n",
    "print(\"Downloading initial dataset...\")\n",
    "response = requests.get(initial_file_path)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Load as Excel\n",
    "    initial_df = pd.read_excel(BytesIO(response.content), engine=\"openpyxl\")\n",
    "    print(\"Initial dataset loaded successfully!\")\n",
    "    display(initial_df.head())\n",
    "\n",
    "    # Create the standardized format\n",
    "    initial_dataset = pd.DataFrame({\n",
    "        'text': initial_df['Statement'],\n",
    "        'sentiment': initial_df['Sentiment'],\n",
    "        'source': 'initial_dataset'\n",
    "    })\n",
    "\n",
    "    print(f\"Loaded {len(initial_dataset)} records from initial dataset\")\n",
    "\n",
    "    # Check sentiment distribution\n",
    "    print(\"\\nSentiment distribution in initial dataset:\")\n",
    "    print(initial_dataset['sentiment'].value_counts())\n",
    "else:\n",
    "    print(f\"Error importing initial dataset: {response.status_code}\")\n",
    "    initial_dataset = pd.DataFrame(columns=['text', 'sentiment', 'source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbJigvre-g6l"
   },
   "source": [
    "## 3. Create Synthetic Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "ZiovskU5-g6l",
    "outputId": "86244b95-69e8-46ab-ae22-7c31d4aff650"
   },
   "outputs": [],
   "source": [
    "# Create synthetic data to augment our dataset\n",
    "def create_synthetic_examples(base_examples, n_variations=3):\n",
    "    \"\"\"\n",
    "    Create synthetic examples by adding variations to existing examples\n",
    "    \"\"\"\n",
    "    if len(base_examples) == 0:\n",
    "        print(\"No base examples provided for synthetic data generation.\")\n",
    "        return pd.DataFrame(columns=['text', 'sentiment', 'source'])\n",
    "\n",
    "    # Define modifiers by sentiment\n",
    "    thai_pos_modifiers = ['มาก', 'มากๆ', 'สุดๆ', 'ที่สุด', 'จริงๆ', 'เหลือเกิน', 'สุดยอด', 'ดีเยี่ยม']\n",
    "    thai_neu_modifiers = ['พอใช้', 'ปานกลาง', 'ธรรมดา', 'ทั่วไป', 'พอได้', 'ไม่เลว']\n",
    "    thai_neg_modifiers = ['แย่', 'แย่มาก', 'ไม่ดี', 'ห่วย', 'แย่สุดๆ', 'เลวร้าย', 'ผิดหวัง']\n",
    "\n",
    "    eng_pos_modifiers = ['very good', 'excellent', 'fantastic', 'amazing', 'great', 'love it']\n",
    "    eng_neu_modifiers = ['okay', 'fine', 'standard', 'acceptable', 'average', 'not bad']\n",
    "    eng_neg_modifiers = ['bad', 'terrible', 'awful', 'horrible', 'disappointing', 'poor']\n",
    "\n",
    "    # Create new examples\n",
    "    new_examples = []\n",
    "\n",
    "    for _, row in base_examples.iterrows():\n",
    "        text = row['text']\n",
    "        sentiment = row['sentiment']\n",
    "\n",
    "        # Create variations\n",
    "        for _ in range(n_variations):\n",
    "            # Choose appropriate modifiers based on sentiment\n",
    "            if sentiment == 'Positive':\n",
    "                thai_mod = np.random.choice(thai_pos_modifiers)\n",
    "                eng_mod = np.random.choice(eng_pos_modifiers)\n",
    "            elif sentiment == 'Neutral':\n",
    "                thai_mod = np.random.choice(thai_neu_modifiers)\n",
    "                eng_mod = np.random.choice(eng_neu_modifiers)\n",
    "            else:  # Negative\n",
    "                thai_mod = np.random.choice(thai_neg_modifiers)\n",
    "                eng_mod = np.random.choice(eng_neg_modifiers)\n",
    "\n",
    "            # Create new text by appending modifiers\n",
    "            # 50% chance for Thai modifier, 50% for English\n",
    "            if np.random.random() > 0.5:\n",
    "                new_text = f\"{text} {thai_mod}\"\n",
    "            else:\n",
    "                new_text = f\"{text} {eng_mod}\"\n",
    "\n",
    "            new_examples.append({\n",
    "                'text': new_text,\n",
    "                'sentiment': sentiment,\n",
    "                'source': 'synthetic'\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(new_examples)\n",
    "\n",
    "# Generate synthetic examples based on initial dataset\n",
    "synthetic_df = create_synthetic_examples(initial_dataset, n_variations=5)\n",
    "\n",
    "print(f\"Created {len(synthetic_df)} synthetic examples\")\n",
    "print(\"\\nSentiment distribution in synthetic dataset:\")\n",
    "print(synthetic_df['sentiment'].value_counts())\n",
    "display(synthetic_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtEBVD_U-g6m"
   },
   "source": [
    "## 4. Create English Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "jaFx5Iwa-g6m",
    "outputId": "7b3991c0-7808-4a8c-d145-c5e617223dd0"
   },
   "outputs": [],
   "source": [
    "# Create English sentiment examples\n",
    "def create_english_examples():\n",
    "    \"\"\"\n",
    "    Create a small dataset of English sentiment examples\n",
    "    \"\"\"\n",
    "    # Define examples for each sentiment category\n",
    "    positive_examples = [\n",
    "        \"This is excellent!\",\n",
    "        \"I love this product.\",\n",
    "        \"Great service and fast delivery.\",\n",
    "        \"The quality is amazing.\",\n",
    "        \"Very satisfied with my purchase.\",\n",
    "        \"Fantastic experience overall.\",\n",
    "        \"The staff was very helpful.\",\n",
    "        \"I would definitely recommend this.\",\n",
    "        \"Exceeded my expectations.\",\n",
    "        \"Best product I've ever bought.\"\n",
    "    ]\n",
    "\n",
    "    neutral_examples = [\n",
    "        \"It's okay.\",\n",
    "        \"Average quality for the price.\",\n",
    "        \"Neither good nor bad.\",\n",
    "        \"It works as expected.\",\n",
    "        \"Standard product, nothing special.\",\n",
    "        \"Acceptable but could be better.\",\n",
    "        \"Does the job, but that's about it.\",\n",
    "        \"Reasonable for the price.\",\n",
    "        \"It's fine I guess.\",\n",
    "        \"Not bad, not great either.\"\n",
    "    ]\n",
    "\n",
    "    negative_examples = [\n",
    "        \"Very disappointed with this.\",\n",
    "        \"Poor quality product.\",\n",
    "        \"Would not recommend at all.\",\n",
    "        \"Terrible customer service.\",\n",
    "        \"Doesn't work as advertised.\",\n",
    "        \"Complete waste of money.\",\n",
    "        \"The worst experience I've had.\",\n",
    "        \"Very frustrated with this purchase.\",\n",
    "        \"Extremely poor quality control.\",\n",
    "        \"I regret buying this.\"\n",
    "    ]\n",
    "\n",
    "    # Create dataframe\n",
    "    data = []\n",
    "\n",
    "    # Add positive examples\n",
    "    for text in positive_examples:\n",
    "        data.append({'text': text, 'sentiment': 'Positive', 'source': 'english_examples'})\n",
    "\n",
    "    # Add neutral examples\n",
    "    for text in neutral_examples:\n",
    "        data.append({'text': text, 'sentiment': 'Neutral', 'source': 'english_examples'})\n",
    "\n",
    "    # Add negative examples\n",
    "    for text in negative_examples:\n",
    "        data.append({'text': text, 'sentiment': 'Negative', 'source': 'english_examples'})\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create English examples\n",
    "english_df = create_english_examples()\n",
    "\n",
    "print(f\"Created {len(english_df)} English examples\")\n",
    "print(\"\\nSentiment distribution in English dataset:\")\n",
    "print(english_df['sentiment'].value_counts())\n",
    "display(english_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxI8mDj_-g6m"
   },
   "source": [
    "## 5. Create Mixed Thai-English Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "IP_5YRLy-g6n",
    "outputId": "dbfb6aa8-42a6-4863-ec7b-6df599bf0015"
   },
   "outputs": [],
   "source": [
    "def create_mixed_examples():\n",
    "    \"\"\"\n",
    "    Create examples with mixed Thai and English text\n",
    "    \"\"\"\n",
    "    # Thai phrases by sentiment\n",
    "    thai_positive = [\n",
    "        \"สินค้าดี\", \"คุณภาพดี\", \"บริการดีมาก\", \"ชอบมาก\", \"ประทับใจ\",\n",
    "        \"คุ้มค่าเงิน\", \"รู้สึกดี\", \"พอใจมาก\", \"เยี่ยมมาก\", \"ดีเลิศ\"\n",
    "    ]\n",
    "\n",
    "    thai_neutral = [\n",
    "        \"พอใช้ได้\", \"ปานกลาง\", \"ธรรมดา\", \"ก็โอเค\", \"เฉยๆ\",\n",
    "        \"ไม่เลวนะ\", \"ทั่วไป\", \"พอทำเนา\", \"ไม่แย่\", \"พอได้\"\n",
    "    ]\n",
    "\n",
    "    thai_negative = [\n",
    "        \"ไม่ดีเลย\", \"แย่มาก\", \"ผิดหวัง\", \"ไม่พอใจ\", \"คุณภาพต่ำ\",\n",
    "        \"บริการแย่\", \"เสียดายเงิน\", \"ไม่ประทับใจ\", \"ราคาแพงเกินไป\", \"ใช้งานยาก\"\n",
    "    ]\n",
    "\n",
    "    # English phrases by sentiment\n",
    "    eng_positive = [\n",
    "        \"great product\", \"excellent service\", \"love it\", \"highly recommend\", \"fantastic\",\n",
    "        \"amazing experience\", \"very satisfied\", \"worth the money\", \"impressive\", \"top quality\"\n",
    "    ]\n",
    "\n",
    "    eng_neutral = [\n",
    "        \"it's okay\", \"average\", \"standard quality\", \"not bad\", \"as expected\",\n",
    "        \"mediocre\", \"acceptable\", \"fair enough\", \"moderate\", \"reasonable\"\n",
    "    ]\n",
    "\n",
    "    eng_negative = [\n",
    "        \"disappointing\", \"poor quality\", \"would not recommend\", \"terrible\", \"waste of money\",\n",
    "        \"very frustrating\", \"doesn't work well\", \"not worth it\", \"bad experience\", \"regret buying\"\n",
    "    ]\n",
    "\n",
    "    # Create mixed examples\n",
    "    mixed_data = []\n",
    "\n",
    "    # Thai first, then English\n",
    "    for thai in thai_positive:\n",
    "        for eng in eng_positive[:3]:  # Limit combinations to avoid too many examples\n",
    "            mixed_data.append({\n",
    "                'text': f\"{thai} {eng}\",\n",
    "                'sentiment': 'Positive',\n",
    "                'source': 'mixed_examples'\n",
    "            })\n",
    "\n",
    "    for thai in thai_neutral:\n",
    "        for eng in eng_neutral[:3]:\n",
    "            mixed_data.append({\n",
    "                'text': f\"{thai} {eng}\",\n",
    "                'sentiment': 'Neutral',\n",
    "                'source': 'mixed_examples'\n",
    "            })\n",
    "\n",
    "    for thai in thai_negative:\n",
    "        for eng in eng_negative[:3]:\n",
    "            mixed_data.append({\n",
    "                'text': f\"{thai} {eng}\",\n",
    "                'sentiment': 'Negative',\n",
    "                'source': 'mixed_examples'\n",
    "            })\n",
    "\n",
    "    # English first, then Thai\n",
    "    for eng in eng_positive:\n",
    "        for thai in thai_positive[:3]:\n",
    "            mixed_data.append({\n",
    "                'text': f\"{eng} {thai}\",\n",
    "                'sentiment': 'Positive',\n",
    "                'source': 'mixed_examples'\n",
    "            })\n",
    "\n",
    "    for eng in eng_neutral:\n",
    "        for thai in thai_neutral[:3]:\n",
    "            mixed_data.append({\n",
    "                'text': f\"{eng} {thai}\",\n",
    "                'sentiment': 'Neutral',\n",
    "                'source': 'mixed_examples'\n",
    "            })\n",
    "\n",
    "    for eng in eng_negative:\n",
    "        for thai in thai_negative[:3]:\n",
    "            mixed_data.append({\n",
    "                'text': f\"{eng} {thai}\",\n",
    "                'sentiment': 'Negative',\n",
    "                'source': 'mixed_examples'\n",
    "            })\n",
    "\n",
    "    # Take a sample to avoid too many examples\n",
    "    if len(mixed_data) > 300:\n",
    "        import random\n",
    "        random.shuffle(mixed_data)\n",
    "        mixed_data = mixed_data[:300]\n",
    "\n",
    "    return pd.DataFrame(mixed_data)\n",
    "\n",
    "# Create mixed examples\n",
    "mixed_df = create_mixed_examples()\n",
    "\n",
    "print(f\"Created {len(mixed_df)} mixed Thai-English examples\")\n",
    "print(\"\\nSentiment distribution in mixed dataset:\")\n",
    "print(mixed_df['sentiment'].value_counts())\n",
    "display(mixed_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlJOQVe9-g6n"
   },
   "source": [
    "## 6. Try to Get Wisesight Dataset (Optional)\n",
    "\n",
    "If this fails, the notebook will continue with other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "meziy2Kq-g6n",
    "outputId": "e3d040a3-900f-4786-993a-40df938c2b05"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    !git clone https://github.com/PyThaiNLP/wisesight-sentiment.git\n",
    "\n",
    "    # Load the datasets\n",
    "    wisesight_data = []\n",
    "    for split in ['train', 'valid']:\n",
    "        try:\n",
    "            file_path = f'wisesight-sentiment/corpus/wisesight_sentiment_{split}.txt'\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()\n",
    "\n",
    "                for i in range(0, len(lines), 2):\n",
    "                    if i+1 < len(lines):\n",
    "                        sentiment = lines[i].strip()\n",
    "                        text = lines[i+1].strip()\n",
    "\n",
    "                        # Map sentiment\n",
    "                        if sentiment == 'pos':\n",
    "                            sentiment = 'Positive'\n",
    "                        elif sentiment == 'neu':\n",
    "                            sentiment = 'Neutral'\n",
    "                        elif sentiment == 'neg':\n",
    "                            sentiment = 'Negative'\n",
    "                        else:\n",
    "                            continue  # Skip questions or other\n",
    "\n",
    "                        wisesight_data.append({\n",
    "                            'text': text,\n",
    "                            'sentiment': sentiment,\n",
    "                            'source': f'wisesight_{split}'\n",
    "                        })\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Wisesight {split} set: {e}\")\n",
    "\n",
    "    # Create dataframe\n",
    "    if wisesight_data:\n",
    "        wisesight_df = pd.DataFrame(wisesight_data)\n",
    "        # Take a sample to keep dataset balanced\n",
    "        wisesight_df = wisesight_df.sample(min(1000, len(wisesight_df)), random_state=42)\n",
    "        print(f\"Loaded {len(wisesight_df)} records from Wisesight dataset\")\n",
    "        print(\"\\nSentiment distribution in Wisesight dataset:\")\n",
    "        print(wisesight_df['sentiment'].value_counts())\n",
    "    else:\n",
    "        print(\"No data loaded from Wisesight\")\n",
    "        wisesight_df = pd.DataFrame(columns=['text', 'sentiment', 'source'])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Wisesight dataset: {e}\")\n",
    "    wisesight_df = pd.DataFrame(columns=['text', 'sentiment', 'source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iIyFHXS-g6n"
   },
   "source": [
    "## 7. Combine All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 871
    },
    "id": "k06ZarY8-g6o",
    "outputId": "b3f98afb-9f78-4425-beed-190887505f40"
   },
   "outputs": [],
   "source": [
    "# Prepare datasets with standardized columns\n",
    "datasets = [\n",
    "    initial_dataset[['text', 'sentiment', 'source']],\n",
    "    synthetic_df[['text', 'sentiment', 'source']],\n",
    "    english_df[['text', 'sentiment', 'source']],\n",
    "    mixed_df[['text', 'sentiment', 'source']],\n",
    "]\n",
    "\n",
    "# Add Wisesight if available\n",
    "if 'wisesight_df' in locals() and len(wisesight_df) > 0:\n",
    "    datasets.append(wisesight_df[['text', 'sentiment', 'source']])\n",
    "\n",
    "# Combine all datasets\n",
    "combined_df = pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "# Remove any rows with missing values\n",
    "combined_df = combined_df.dropna()\n",
    "\n",
    "# Basic cleaning\n",
    "combined_df['text'] = combined_df['text'].astype(str).apply(lambda x: x.strip())\n",
    "\n",
    "# Remove duplicate texts\n",
    "combined_df = combined_df.drop_duplicates(subset=['text'])\n",
    "\n",
    "print(f\"Combined dataset: {len(combined_df)} records\")\n",
    "print(\"\\nSentiment distribution in combined dataset:\")\n",
    "print(combined_df['sentiment'].value_counts())\n",
    "print(\"\\nSource distribution:\")\n",
    "print(combined_df['source'].value_counts())\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(y='sentiment', data=combined_df, palette='viridis')\n",
    "plt.title('Sentiment Distribution in Combined Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOMdMdsr-g6o"
   },
   "source": [
    "## 8. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOMAku0H-g6o",
    "outputId": "a64edac2-2563-43d6-ccb6-52c8b0666bf0"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text data by removing URLs, user mentions, extra spaces, etc.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove user mentions (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "combined_df['cleaned_text'] = combined_df['text'].apply(clean_text)\n",
    "\n",
    "# Custom tokenizer for Thai + English\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize text using pythainlp for Thai language support\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # Tokenize with pythainlp's newmm tokenizer\n",
    "    try:\n",
    "        tokens = word_tokenize(text, engine='newmm')\n",
    "        # Remove tokens that are just whitespace or single characters\n",
    "        tokens = [token for token in tokens if token.strip() and len(token.strip()) > 1]\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing text: {text}, Error: {e}\")\n",
    "        # Fall back to basic split\n",
    "        return [t for t in text.split() if len(t.strip()) > 1]\n",
    "\n",
    "# Test the tokenizer on a mixed language example\n",
    "test_text = \"สินค้าดีมาก I really like this product นี่เป็นการซื้อครั้งที่ 2 แล้ว will buy again\"\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Tokenized: {tokenize_text(test_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1tY5e_W-g6o"
   },
   "source": [
    "## 9. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQUHvb0l-g6o",
    "outputId": "bf437950-6679-467b-f337-34234f37f328"
   },
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X = combined_df['cleaned_text']\n",
    "y = combined_df['sentiment']\n",
    "\n",
    "# Split into training and testing sets (stratified by sentiment)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} examples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pjGboAnU-g6o",
    "outputId": "8859887f-b2f4-4ec0-e645-639b7e5ecb95"
   },
   "outputs": [],
   "source": [
    "# Create a pipeline with TF-IDF and classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        tokenizer=tokenize_text,\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=30000,\n",
    "        min_df=2,\n",
    "        sublinear_tf=True  # Apply sublinear tf scaling (log)\n",
    "    )),\n",
    "    ('classifier', LogisticRegression(\n",
    "        C=1.0,\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced',\n",
    "        solver='liblinear',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "%time pipeline.fit(X_train, y_train)\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkvJcaFC-g6p"
   },
   "source": [
    "## 10. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "id": "kmrdiTYm-g6p",
    "outputId": "fee90b5c-5a62-4459-dfd4-ba4cc5ccc0b8"
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=pipeline.classes_,\n",
    "           yticklabels=pipeline.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "fDCpBEVW-g6p",
    "outputId": "cd1c288c-b0b7-44fe-8956-6bac2f2c36c6"
   },
   "outputs": [],
   "source": [
    "# Evaluate on different data sources\n",
    "source_results = {}\n",
    "\n",
    "# Test on each source\n",
    "for source in combined_df['source'].unique():\n",
    "    source_data = combined_df[combined_df['source'] == source]\n",
    "    if len(source_data) > 10:  # Only test if we have enough examples\n",
    "        X_source = source_data['cleaned_text']\n",
    "        y_source = source_data['sentiment']\n",
    "\n",
    "        # Predict\n",
    "        y_source_pred = pipeline.predict(X_source)\n",
    "        source_accuracy = accuracy_score(y_source, y_source_pred)\n",
    "        source_results[source] = source_accuracy\n",
    "\n",
    "# Display results by source\n",
    "print(\"Accuracy by data source:\")\n",
    "for source, acc in source_results.items():\n",
    "    print(f\"{source}: {acc:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(source_results.keys()), y=list(source_results.values()))\n",
    "plt.title('Model Accuracy by Data Source')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Data Source')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iOVJKYN-g6p"
   },
   "source": [
    "## 11. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "pVYgm9ko-g6q",
    "outputId": "076a6fcf-9b06-4f1e-f8b9-a76e6942c575"
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_filename = 'thai_english_sentiment_model.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "print(f\"Model saved as '{model_filename}'\")\n",
    "\n",
    "# Save the combined dataset for future use\n",
    "combined_df.to_csv('thai_english_sentiment_dataset.csv', index=False)\n",
    "print(\"Dataset saved as 'thai_english_sentiment_dataset.csv'\")\n",
    "\n",
    "# Download files to your computer\n",
    "from google.colab import files\n",
    "files.download(model_filename)\n",
    "files.download('thai_english_sentiment_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0q_jyZg-g6q"
   },
   "source": [
    "## 12. Test with Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m81kN6GL-g6q",
    "outputId": "4559bc05-0806-4f84-b606-776ebaf94ae0"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model=pipeline):\n",
    "    \"\"\"\n",
    "    Predict sentiment for text in Thai, English, or mixed.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        model: Trained model pipeline\n",
    "\n",
    "    Returns:\n",
    "        tuple: (predicted sentiment, confidence percentage, all probabilities)\n",
    "    \"\"\"\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict([cleaned_text])[0]\n",
    "\n",
    "    # Get prediction probabilities\n",
    "    proba = model.predict_proba([cleaned_text])[0]\n",
    "    confidence = max(proba) * 100\n",
    "\n",
    "    # Get all class probabilities with labels\n",
    "    all_probas = {cls: prob*100 for cls, prob in zip(model.classes_, proba)}\n",
    "\n",
    "    return prediction, confidence, all_probas\n",
    "\n",
    "# Test with examples\n",
    "test_examples = [\n",
    "    # Thai examples\n",
    "    \"สินค้าดีมากๆ คุณภาพเยี่ยม\",                        # Very good product, excellent quality\n",
    "    \"พอใช้ได้ ไม่ดีมาก ไม่แย่มาก\",                      # It's okay, not very good, not very bad\n",
    "    \"แย่มาก ผิดหวังมากกับสินค้าตัวนี้\",                 # Very bad, very disappointed with this product\n",
    "    # English examples\n",
    "    \"This product is amazing, exceeded my expectations!\",\n",
    "    \"It's an average product, does what it's supposed to do.\",\n",
    "    \"Terrible service, would not recommend to anyone.\",\n",
    "    # Mixed Thai-English examples\n",
    "    \"ชอบสินค้านี้มาก Best purchase ever!\",             # Love this product, best purchase ever\n",
    "    \"Product is okay, แต่ราคาแพงไปนิด\",                # Product is okay, but a bit expensive\n",
    "    \"Disappointed with the quality ใช้แล้วพังเลย\"      # Disappointed with the quality, it broke after using\n",
    "]\n",
    "\n",
    "# Test and display results\n",
    "results = []\n",
    "for text in test_examples:\n",
    "    sentiment, confidence, all_probas = predict_sentiment(text)\n",
    "    results.append({\n",
    "        'text': text,\n",
    "        'predicted_sentiment': sentiment,\n",
    "        'confidence': f\"{confidence:.2f}%\",\n",
    "        'all_probabilities': all_probas\n",
    "    })\n",
    "\n",
    "# Display results in a formatted way\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Predicted Sentiment: {result['predicted_sentiment']}\")\n",
    "    print(f\"Confidence: {result['confidence']}\")\n",
    "    print(\"Class Probabilities:\")\n",
    "    for cls, prob in result['all_probabilities'].items():\n",
    "        print(f\"  - {cls}: {prob:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjCAumQB-g6q"
   },
   "source": [
    "## 13. Interactive Testing Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iuf1gIWD-g6r",
    "outputId": "389b3ff6-1869-4797-aaf1-3f9a9a427fb0"
   },
   "outputs": [],
   "source": [
    "def analyze_text():\n",
    "    text = input(\"Enter text (Thai/English/Mixed) to analyze (or 'q' to quit): \")\n",
    "    if text.lower() == 'q':\n",
    "        return False\n",
    "\n",
    "    if text.strip() == '':\n",
    "        print(\"Please enter some text to analyze.\")\n",
    "        return True\n",
    "\n",
    "    sentiment, confidence, all_probas = predict_sentiment(text)\n",
    "\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n",
    "    print(f\"Confidence: {confidence:.2f}%\")\n",
    "    print(\"Class Probabilities:\")\n",
    "    for cls, prob in all_probas.items():\n",
    "        print(f\"  - {cls}: {prob:.2f}%\")\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    return True\n",
    "\n",
    "print(\"=== Thai-English Sentiment Analysis Tool ===\\n\")\n",
    "print(\"Examples you can try:\")\n",
    "print(\"1. สินค้าดีมากๆ คุณภาพเยี่ยม (Very good product, excellent quality)\")\n",
    "print(\"2. Product is okay, แต่ราคาแพงไปนิด (Product is okay, but a bit expensive)\")\n",
    "print(\"3. Terrible service, never again!\")\n",
    "print(\"\\nType 'q' to quit when you're done.\\n\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "continue_analysis = True\n",
    "while continue_analysis:\n",
    "    continue_analysis = analyze_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5z53_fJw-g6r"
   },
   "source": [
    "## 14. Next Steps: How to Expand Your Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr1ESKHe-g6r",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### เพิ่มข้อมูลในชุดข้อมูลของคุณ\n",
    "\n",
    "1. **Web Scraping** - เก็บข้อมูลจากเว็บไซต์รีวิวภาษาไทย เช่น:\n",
    "   - Pantip\n",
    "   - Wongnai\n",
    "   - Twitter (ใช้ Twitter API)\n",
    "   - Facebook Comments (ใช้ Facebook Graph API)\n",
    "\n",
    "2. **ใช้ Dataset สาธารณะ** - ชุดข้อมูลเพิ่มเติมที่สามารถดาวน์โหลดได้:\n",
    "   - [VISTEC-TP-TH-Twitter-Sentiment](https://github.com/mrpeerat/VISTEC-TP-TH-Twitter-Sentiment) - ชุดข้อมูลความรู้สึกภาษาไทยบน Twitter\n",
    "   - [Thai-Social-Media-Crisis-Dataset](https://github.com/jitkapat/Thai-Social-Media-Crisis-Dataset) - ชุดข้อมูลวิกฤตในสื่อสังคมไทย\n",
    "   - [Thai Review and Rating Dataset](https://github.com/PyThaiNLP/thai-review-and-rating-dataset) - ชุดข้อมูลรีวิวและคะแนนภาษาไทย\n",
    "\n",
    "3. **สร้างข้อมูลด้วย AI** - ใช้ AI สร้างข้อมูลเพิ่มเติม:\n",
    "   - ใช้ API ของ ChatGPT หรือ GPT-4 เพื่อสร้างข้อความตัวอย่างมากขึ้น\n",
    "   - ใช้การแปลภาษาเพื่อแปลชุดข้อมูลภาษาอังกฤษที่มีอยู่เป็นภาษาไทย\n",
    "\n",
    "4. **Data Augmentation** - เทคนิคการเพิ่มข้อมูล:\n",
    "   - แทนที่คำด้วยคำที่มีความหมายเหมือนกัน\n",
    "   - สลับตำแหน่งประโยค\n",
    "   - เพิ่มหรือลบคำที่ไม่สำคัญ\n",
    "\n",
    "### พัฒนาโมเดลให้ดีขึ้น\n",
    "\n",
    "1. **ใช้ Deep Learning** - โมเดลที่ซับซ้อนมากขึ้น:\n",
    "   - [wangchanberta](https://huggingface.co/airesearch/wangchanberta-base-att-spm-uncased) - BERT สำหรับภาษาไทย\n",
    "   - [XLM-RoBERTa](https://huggingface.co/xlm-roberta-base) - สนับสนุนหลายภาษา\n",
    "   - [mBERT](https://huggingface.co/bert-base-multilingual-cased) - BERT หลายภาษา\n",
    "\n",
    "2. **การใช้ Transformer Models** - ตัวอย่างโค้ด:\n",
    "```python\n",
    "# ติดตั้งไลบรารี\n",
    "!pip install transformers torch\n",
    "\n",
    "# โค้ดตัวอย่างการใช้ wangchanberta\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# โหลดโมเดลและ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\", num_labels=3)\n",
    "\n",
    "# ฟังก์ชันทำนาย\n",
    "def predict_with_transformer(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return probabilities\n",
    "```\n",
    "\n",
    "### แหล่งข้อมูลเพิ่มเติม\n",
    "\n",
    "1. [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp) - ไลบรารีประมวลผลภาษาธรรมชาติสำหรับภาษาไทย\n",
    "2. [Thai NLP Resource](https://github.com/kobkrit/thai-nlp-resource) - รวมลิงก์ข้อมูลและเครื่องมือสำหรับ NLP ภาษาไทย\n",
    "3. [VISTEC-AI Research](https://github.com/vistec-AI) - งานวิจัย AI และ NLP สำหรับภาษาไทย"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
